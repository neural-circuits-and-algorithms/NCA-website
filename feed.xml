<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://neural-circuits-and-algorithms.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://neural-circuits-and-algorithms.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-01-07T17:14:21-05:00</updated><id>https://neural-circuits-and-algorithms.github.io/feed.xml</id><title type="html">Neural Circuits and Algorithms</title><subtitle>Neural Circuits and Algorithms Group in the Center for Computational Neuroscience at the Flatiron Institute
</subtitle><entry><title type="html">The Search for Biologically Plausible Neural Computation — Part 2</title><link href="https://neural-circuits-and-algorithms.github.io/blog/2018/bio-conventional2/" rel="alternate" type="text/html" title="The Search for Biologically Plausible Neural Computation — Part 2" /><published>2018-12-03T00:00:00-05:00</published><updated>2018-12-03T00:00:00-05:00</updated><id>https://neural-circuits-and-algorithms.github.io/blog/2018/bio-conventional2</id><content type="html" xml:base="https://neural-circuits-and-algorithms.github.io/blog/2018/bio-conventional2/"><![CDATA[<p>This is the second post in a series reviewing recent progress in designing artificial neural networks (NNs) that resemble natural NNs not just superficially, but on a deeper, algorithmic level. In addition to serving as models of natural NNs, such networks can serve as general-purpose machine learning algorithms. Respecting biological constraints, viewed naively as a handicap in developing competitive general-purpose machine learning algorithms, can instead facilitate the development of artificial NNs by restricting the search space of possible algorithms.</p>

<p>In the previous post, we focused on the constraints that must be met for an unsupervised algorithm to be biologically plausible. For an algorithm to be implementable as a NN, it must be formulated in the online setting. In the corresponding NN, synaptic weight updates must be local, i.e. depend on the activity of only two neurons that the synapse connects. Then, we demonstrated that deriving NNs for dimensionality reduction in a conventional way - by minimizing the reconstruction error - results in multi-neuron networks with biologically implausible non-local learning rules.</p>

<p>In this post, we propose a different objective function which we term similarity matching. From this objective function, we derive an online algorithm implementable by a NN with local learning rules. Then, we introduce other similarity-based algorithms which include more biological features such as different neuron classes and nonlinear activation functions. Finally, we review similarity-matching algorithms with state-of-the-art performance.</p>

<h2 id="similarity-matching-objective-function">Similarity-matching objective function</h2>

<p>We start by stating an objective function that will be used to derive NNs for linear dimensionality reduction. Let ${\bf x_t} \in R^n, t=1,…T$, be a set of data points (inputs) and ${\bf y_t} \in R^k, t=1,…T, (k&lt;n)$ be their learned representation (outputs). The similarity of a pair of inputs, ${\bf x_t}$ and ${\bf x_t '}$, can be defined as their dot-product, ${\bf x_t}$⊤${\bf x_t'}$. Analogously, the similarity of a pair of outputs is {\bf y_t}⊤{\bf y_t}′. Similarity matching, as its name suggests, learns a representation where the similarity between each pair of outputs matches that of the corresponding inputs:</p>

\[\min_{ {\bf y}_1,\ldots,{\bf y}_T}  \frac{1}{T^2}  \sum_{t=1}^T  \sum_{t'=1}^T \left({\bf x}_t^\top {\bf x}_{t'} - {\bf y}_t^\top {\bf y}_{t'} \right)^2. \qquad\qquad (2.1)\]

<p>This offline objective function, previously employed for multidimensional scaling, is optimized by the projections of inputs onto the principal subspace of their covariance, i.e. performing PCA up to an orthogonal rotation. Moreover, (2.1) has no local minima other than the principal subspace solution.</p>

<p>The similarity-matching objective (2.1) may seem like a strange choice for deriving an online algorithm implementable by a NN. In the online setting, inputs are streamed to the algorithm sequentially and each output must be computed before the ne${\bf x_t}$ input arrives. Yet, in (2.1), pairs of inputs and outputs from different time points interact with each other. In addition, whereas ${\bf x_t}$ and ${\bf y_t}$ could be interpreted as inputs and outputs to a network, unlike in the reconstruction approach (1.4), synaptic weights do not appear explicitly in (2.1).</p>

<h2 id="variable-substitution-trick">Variable substitution trick</h2>
<p>Both of the above concerns can be resolved by a simple math trick akin to completing the square. We first focus on the cross-term in (2.1), which we call similarity alignment. By re-ordering the variables and introducing a new variable, ${\bf W} \in R^{k \times n}$, we obtain:</p>

\[- \frac{1}{T^2}\sum_{t=1}^T \sum_{t'=1}^T  {\bf y}_{t}^\top {\bf y}_{t'} {\bf x}_t^\top {\bf x}_{t'}= - \frac{1}{T^2}\sum_{t=1}^T  {\bf y}_{t}^\top  \left[ \sum_{t'=1}^T {\bf y}_{t'} {\bf x}_{t'}^\top \right ] {\bf x}_t\]

\[\qquad \qquad \qquad \qquad\qquad  =   \min_{ {\bf W} \in \mathbb{R}^{k\times n}}  -\frac{2}{T} \sum_{t=1}^T{\bf y}_t^\top {\bf W} {\bf x}_t + {\rm Tr } {\bf W}^\top {\bf W}.\; (2.2)\]

<p>To prove the second identity, find optimal ${\bf W}$ by taking a derivative of the expression on the right with respect to ${\bf W}$ and setting it to zero, and then substitute the optimal ${\bf W}$ back into the expression. Similarly, for the quartic ${\bf y_t}$ term in (2.1):</p>

\[\frac{1}{T^2}\sum_{t=1}^T \sum_{t'=1}^T  {\bf y}_{t}^\top {\bf y}_{t'} {\bf y}_t^\top {\bf y}_{t'}= \frac{1}{T^2}\sum_{t=1}^T  {\bf y}_{t}^\top  \left[ \sum_{t'=1}^T {\bf y}_{t'} {\bf y}_{t'}^\top \right ] {\bf y}_t\]

\[\qquad \qquad \qquad \qquad\qquad  =   \max_{ {\bf M} \in \mathbb{R}^{k\times k}}  \frac{2}{T} \sum_{t=1}^T{\bf y}_t^\top {\bf M} {\bf y}_t - {\rm Tr }  {\bf M}^\top {\bf M}.\quad(2.3)\]

<p>By substituting (2.2) and (2.3) into (2.1) we get:</p>

\[\min_{ {\bf W}\in \mathbb{R}^{k\times n}}\max_{ {\bf M}\in \mathbb{R}^{k\times k}}  \frac{1}{T} \sum_{t=1}^T \left[2 {\rm Tr}\left({\bf W}^\top{\bf W}\right) - {\rm Tr}\left({\bf M}^\top{\bf M}\right) +  \min_{ {\bf y}_t\in \mathbb{R}^{k\times 1}}  l_t({\bf W},{\bf M},{\bf y}_t)\right],\; (2.4)\]

<p>where</p>

\[l_t({\bf W},{\bf M},{\bf y}_t)=-4{\bf x}_t^\top{\bf W}{\bf y}_t + 2{\bf y}_t^\top{\bf M}{\bf y}_t.\qquad\qquad (2.5)\]

<p>In the resulting objective function, (2.4),(2.5), optimal outputs at different time steps can be computed independently, making the problem amenable to an online algorithm. The price paid for this simplification is the appearance of the minimax optimization problem in variables, ${\bf W}$ and ${\bf M}$. Minimization over  ${\bf W}$ aligns output channels with the greatest variance directions of the input and maximization over ${\bf M}$ diversifies the output channels. The competition between the two in a gradient descent/ascent algorithm results in the principal subspace projection which is the only stable fixed point of the corresponding dynamics.</p>

<h2 id="online-algorithm-and-neural-network">Online algorithm and neural network</h2>
<p>Now, we are ready to derive an algorithm for optimizing (2.1) online. First, by minimizing (2.5) with respect to ${\bf y_t}$ while keeping ${\bf W}$ and $ {\bf M} $ fixed we get the dynamics for the output variables:</p>

\[\dot{\bf y}_t={\bf W} {\bf x}_t-{\bf M} {\bf y}_t.\qquad\qquad (2.6)\]

<p>To find ${\bf y_t}$ after the presentation of the corresponding input, ${\bf x_t}$, (2.6) is iterated until convergence.</p>

<p>After the convergence of ${\bf y_t}$ we update ${\bf W}$ and $ {\bf M} $ by gradient descent of (2.2) and gradient ascent of (2.3) respectively:</p>

\[W_{ij} \leftarrow W_{ij} + \eta \left(y_i{\bf x_j}-{\bf W}_{ij}\right), \qquad   M_{ij} \leftarrow M_{ij} + \eta \left(y_iy_j-M_{ij}\right). \qquad (2.7)\]

<p>Algorithm (2.6),(2.7), first derived <a href="https://www.researchgate.net/publication/273003026_A_HebbianAnti-Hebbian_Neural_Network_for_Linear_Subspace_Learning_A_Derivation_from_Multidimensional_Scaling_of_Streaming_Data">here</a>, can be naturally implemented by a biologically plausible NN, Figure 1. Here, activity (firing rate) of the upstream neurons corresponds to input variables. Output variables are computed by the dynamics of activity (2.6) in a single layer of neurons. Variables ${\bf W}$ and $ {\bf M} $ are represented by the weights of synapses in feedforward and lateral connections respectively. The learning rules (2.7) are local, i.e. the weight update, ΔWij, for the synapse between jth input neuron and ith output neuron depends only on the activities, ${\bf x_j}$, of jth input neuron and, ${\bf y_i}$, of ith output neuron, and the synaptic weight. In neuroscience, learning rules (2.7) for ${\bf W}$ and ${\bf M}$ are called Hebbian and anti-Hebbian respectively.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/bio2_Figure1-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/bio2_Figure1-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/bio2_Figure1-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/bio2_Figure1.png" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1: A Hebbian/Anti-Hebbian network derived from similarity matching.
</div>

<p>To summarize, starting with the similarity-matching objective, we derived a Hebbian/anti-Hebbian NN for dimensionality reduction. The minimax objective can be viewed as a zero-sum game played by the weights of feedforward and lateral connections. This demonstrates that synapses with local updates can still collectively work together to optimize a global objective. A similar, although not identical, NN was proposed by Foldiak heuristically. The advantage of our normative approach is that the offline solution is known. Although no proof of convergence exists in the online setting, algorithm (2.6),(2.7) performs well in practice.</p>

<h2 id="other-similarity-based-objectives-and-linear-networks">Other similarity-based objectives and linear networks</h2>
<p>We used the same framework to derive NNs for other computational tasks and incorporating more biological features. As the algorithm (2.6),(2.7) and the NN in Figure 1 were derived from the similarity-matching objective (2.1), they project data onto the principal subspace but do not necessarily recover principal components per se. To derive PCA algorithms we modified the objective function (2.1), <a href="https://arxiv.org/abs/1511.09468">here</a> and (here)[https://arxiv.org/abs/1810.06966], to encourage orthogonality of ${\bf W}$. Such algorithms are implemented by NNs of the same architecture as in Figure 1 but with slightly different learning rules.</p>

<p>Although the similarity-matching NN in Figure 1 relies on biologically plausible local learning rules, it lacks biological realism in several other ways. For example, computing output requires recurrent activity that must settle faster than the time scale of the input variation, which is unlikely in biology. To respect this biological constraint, <a href="https://arxiv.org/abs/1810.06966">we modified</a> the dimensionality reduction algorithm to avoid recurrency.</p>

<p>Another non-biological feature of the NN in Figure 1 is that the output neurons compete with each other by communicating via lateral connections. In biology, such interactions are not direct but mediated by interneurons. To reflect these observations, we modified the objective function by introducing a whitening constraint:</p>

\[\min_{ {\bf y}_1,\ldots,{\bf y}_T}  - \frac{1}{T^2}\sum_{t=1}^T \sum_{t'=1}^T  {\bf y}_{t}^\top {\bf y}_{t'} {\bf x}_t^\top {\bf x}_{t'}, \qquad {\rm s.t.} \quad \frac 1T \sum_{t=1}^T {\bf y}_t {\bf y}_t^\top = {\bf I}_k, \qquad (2.8)\]

<p>where Ik is the k-by-k identity matrix. Then, by representing the whitening constraint using Lagrange relaxation, we derived NNs where interneurons appear naturally - their activity is modeled by the Lagrange multipliers, z⊤tzt′ (Figure 2):</p>

\[\min_{ {\bf y}_1,\ldots,{\bf y}_T} \max_{ {\bf z}_1,\ldots,{\bf z}_T } - \frac{1}{T^2}\sum_{t=1}^T \sum_{t'=1}^T  {\bf y}_{t}^\top {\bf y}_{t'} {\bf x}_t^\top {\bf x}_{t'} + \frac{1}{T^2}\sum_{t=1}^T \sum_{t'=1}^T  {\bf z}_{t}^\top {\bf z}_{t'} \left({\bf y}_{t}^\top {\bf y}_{t'} -\delta_{t,t'}\right). \; (2.9)\]

<p>Notice how (2.9) contains the y-z similarity-alignment term similar to (2.2). We can now derive learning rules for the y-z connections using the variable substitution trick, leading to the network in Figure 2. For details of this and other NN derivations, see <a href="http://papers.nips.cc/paper/5885-a-normative-theory-of-adaptive-dimensionality-reduction-in-neural-networks">here</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/bio2_Figure2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/bio2_Figure2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/bio2_Figure2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/bio2_Figure2.png" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2: A biologically-plausible NN for whitening inputs, derived from a constrained similarity-alignment cost function.
</div>

<h2 id="nonnegative-similarity-matching-objective-and-a-nonlinear-network">Nonnegative similarity-matching objective and a nonlinear network</h2>

<p>So far, we considered similarity-based NNs with linear neurons. However, biological neurons are not linear and many interesting computations require nonlinearity. A resolution to this discrepancy was suggested by the observation that the output of biological neurons is nonnegative (firing rate cannot be below zero). Hence, we modified the optimization problem by requiring that the output of the similarity-matching cost function (2.1) is nonnegative:</p>

\[\min_{ {\bf y}_1,\ldots,{\bf y}_T \geq 0}  \frac{1}{T^2}  \sum_{t=1}^T  \sum_{t'=1}^T \left({\bf x}_t^\top {\bf x}_{t'} - {\bf y}_t^\top {\bf y}_{t'} \right)^2. \qquad\qquad (2.10)\]

<p>Solutions of the optimization problem (2.10) are very different from PCA: They can cluster well-segregated data and extract sparse features from data. Understanding the nature of these solutions will be the topic of the next. post. For now, we note that (2.10) can be solved by the same online algorithm as (2.1) except that the output variables are projected onto the nonnegative domain. Such algorithm maps onto the same network as Figure 1 but with rectifying neurons (ReLUs), Figure 3A.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/bio2_Figure3-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/bio2_Figure3-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/bio2_Figure3-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/bio2_Figure3.png" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3: A) A nonlinear Hebbian/Anti-Hebbian network derived from nonnegative similarity matching. B) Stacked network for NICA. NSM - nonnegative similarity-matching.
</div>

<p>Another problem solved by similarity-based networks is the nonnegative independent component analysis (NICA) which can be used for blind source separation. The problem is to recover independent and nonnegative sources from observing only their linear mixture. <a href="https://www.researchgate.net/publication/3342750_Conditions_for_nonnegative_independent_component_analysis">Plumbley</a> showed that NICA can be solved in two steps, Figure 4. First, whiten the data to obtain an orthogonal rotation of the sources. Second, find an orthogonal rotation of the whitened sources that yields a nonnegative output, Figure 4. The first step can be implemented by the whitening network in Figure 2. The second step can be implemented by the nonnegative similarity-matching network, Figure 3A, because an orthogonal rotation does not affect dot-product similarities. Therefore, NICA is solved by stacking the whitening and the nonnegative similarity-matching networks, Figure 3B.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/bio2_Figure4-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/bio2_Figure4-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/bio2_Figure4-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/bio2_Figure4.png" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 4: Illustration of the nonnegative independent component analysis algorithm. Two source channels (left) are linearly transformed to a two-dimensional mixture, which are the inputs to the algorithm (middle). Whitening (right) yields an orthogonal rotation of the sources. Sources are then recovered by solving the nonnegative similarity-matching problem. Green and red plus signs track two source vectors across mixing and whitening stag
</div>

<h2 id="similarity-based-algorithms-as-general-purpose-tools">Similarity-based algorithms as general-purpose tools</h2>

<p>While the derivation of the similarity-matching algorithm was motivated by constraints imposed by biology, the resulting algorithm performs well on large-scale data. A recent paper introduced an efficient modification of the similarity-matching algorithm and demonstrated its competitiveness with the state-of-the-art principal subspace projection algorithms in both processing speed and convergence rate. A package with implementations of these algorithms is <a href="https://github.com/flatironinstitute/online_psp">here</a> and <a href="https://github.com/flatironinstitute/online_psp_matlab">here</a>.</p>

<p>In this blog post, we introduced linear and non-linear similarity-matching NNs that can serve as models of biological NNs and as general-purpose machine-learning tools. In the ne${\bf x_t}$ post, we will discuss the nature of the solutions to nonnegative similarity-based networks.</p>]]></content><author><name>Dmitri 'Mitya' Chklovskii</name></author><category term="biologically-plausible" /><category term="neural-computation" /><summary type="html"><![CDATA[a similarity-based approach]]></summary></entry><entry><title type="html">The Search for Biologically Plausible Neural Computation — Part 1</title><link href="https://neural-circuits-and-algorithms.github.io/blog/2016/bio-conventional/" rel="alternate" type="text/html" title="The Search for Biologically Plausible Neural Computation — Part 1" /><published>2016-11-03T00:00:00-04:00</published><updated>2016-11-03T00:00:00-04:00</updated><id>https://neural-circuits-and-algorithms.github.io/blog/2016/bio-conventional</id><content type="html" xml:base="https://neural-circuits-and-algorithms.github.io/blog/2016/bio-conventional/"><![CDATA[<p>Inventors of the original artificial neural networks (NNs) derived their inspiration from biology. However, as artificial NNs progressed, their design was less guided by neuroscience facts. Meanwhile, progress in neuroscience has altered our conceptual understanding of neurons. Consequently, we believe that many successful artificial NNs resemble natural NNs only superficially violating fundamental constraints imposed by biological hardware.</p>

<p>The wide gap between the artificial and natural NN designs raises intriguing questions: What algorithms underlie natural NNs? Can insights from biology help build better artificial NNs?</p>

<p>This is the first of a series of posts aimed at explaining recent progress made by my collaborators and myself towards biologically plausible NNs. Such networks can serve both as models of natural NNs and as general purpose artificial NNs. We have found that respecting biological constraints actually helps development of artificial NNs by guiding design decisions.</p>

<p>In this post, I cover the background material, going back several decades. I sketch a biological neuron, introduce primary biological constraints, and discuss the conventional approach to deriving artificial NNs. I will show that while the conventional approach generates a reasonable algorithmic model of a single biological neuron, multi-neuron networks violate biological constraints. In future posts we will see how to fix that.</p>

<h2 id="a-sketch-of-a-biological-neuron">A Sketch of a Biological Neuron</h2>

<p>Here is the minimum biological background needed to understand the rest of the post.</p>

<p>A biological neuron receives signals from multiple neurons, computes their weighted sum and generates a signal transmitted to multiple neurons, Figure 1. Each neuron’s signaling activity is quantified by the firing rate, which is a nonnegative real number that varies over time. Each synapse scales the input from the corresponding upstream neuron onto the receiving neuron by its weight. The receiving neuron sums scaled inputs, i.e. computes the inner product of the upstream activity vector and the synaptic weight vector. The inner product passes through a nonlinearity called the activation function and the output is transmitted to downstream neurons.</p>

<p>Synaptic weight changes over time, typically, on a slower time scale than neuronal signals. The weight depends on neuronal signals per so-called learning rules. For example, in commonly used <a href="https://en.wikipedia.org/wiki/Hebbian_theory">Hebbian learning rules</a>, synaptic weight is proportional to the correlation between the activities of the two neurons a synapse connects, i.e. pre- and postsynaptic.</p>

<h2 id="primary-biological-constraints">Primary Biological Constraints</h2>

<p>To determine which algorithmic models in this post are biologically plausible, we can focus on a few key biological constraints.</p>

<p>Biologically plausible algorithms must be formulated in the online (or streaming), rather than offline (or batch), setting. This means that input data are streamed to the algorithm sequentially, one sample at a time, and the corresponding output must be computed before the next input sample arrives. The output communicated to downstream neurons cannot be modified in the future. A neuron cannot store individual past inputs or outputs except in a highly compressed format limited to synaptic weights and a few state variables.</p>

<p>In biologically plausible NNs, learning rules must be local. This means that the synaptic weight update may depend on the activities of only the two neurons a synapse connects, as for example, in Hebbian learning. Activities of other neurons are not physically available to a synapse and therefore including them into learning rules would be biologically implausible. Modern artificial NNs, such as backpropagation-based deep learning networks, rely on nonlocal learning rules.</p>

<p>Our initial focus is on unsupervised learning. This is not a hard constraint, but rather a matter of priority. Whereas humans are clearly capable of supervised learning, most of our learning tasks lack big labeled datasets. On the mechanistic level, most neurons lack a clear supervision signal.</p>

<h2 id="single-neuron-online-principal-component-analysis-pca">Single-neuron Online Principal Component Analysis (PCA)</h2>

<p>In 1982, <a href="https://www.semanticscholar.org/paper/Simplified-neuron-model-as-a-principal-component-Oja/3e00dd12caea7c4dab1633a35d1da3cb2e76b420?p2df">Oja proposed</a> modeling a neuron by an online PCA algorithm. PCA is a workhorse of data analysis used for dimensionality reduction, denoising, and latent factor discovery. Therefore, Oja’s seminal paper established that biological processes in a neuron can be viewed as the steps of an online algorithm solving a useful computational objective.</p>

<p>Oja’s single-neuron online PCA algorithm works as follows. At each time step, \(t\), it receives an input data sample, \({\bf x}_t\), computes and outputs the corresponding top principal component value, \({\bf y}_t\):</p>

\[y_t \leftarrow {\bf w} _{t-1}^\top {\bf x}_t. \qquad \qquad \qquad (1.1)\]

<p>Here and below lowercase boldfaced letters designate vectors. Then the algorithm updates the (normalized) feature vector,</p>

\[{\bf w} _t \leftarrow {\bf w} _{t-1}+ \eta ( {\bf x} _t- {\bf w} _{t-1} y_t  ) y_t. \qquad \qquad (1.2)\]

<p>The feature vector, w, converges to the eigenvector of input covariance if data are drawn i.i.d from a stationary distribution.</p>

<p>The steps of the Oja algorithm (1.1-1.2) correspond to the operations of the biological neuron. If the input vector is represented by the activities of the upstream neurons, (1.1) represents weighted summation of the inputs by the output neuron. If the activation function is linear the output, \({\bf y}_t\), is simply the weighted sum. The update (1.2) is a local Hebbian synaptic learning rule. The first term of the update is proportional to the correlation of the pre- and postsynaptic neurons’ activities and the second term, also local, normalizes the synaptic weight vector.</p>

<h2 id="a-normative-theory">A Normative Theory</h2>

<p>Next, we would like to build on Oja’s insightful identification of biological processes with the steps of the online PCA algorithm by computing multiple principal components using multi-neuron NNs and including the activation nonlinearity.</p>

<p>Instead of trying to extend the Oja model heuristically, we take a more systematic, so-called normative approach. In this approach, a biological model is viewed as the solution of an optimization problem. Specifically, we postulate an objective function motivated by a computational principle, derive an online algorithm optimizing such objective, and map the steps of the algorithm onto biological processes.</p>

<p>Having such normative theory allows us to navigate through the space of possible algorithmic models in a more efficient and systematic way. Mathematical compactness of objective functions facilitates generating new models and weeding out inconsistent ones. This is similar to the Hamiltonian approach in physics which leverages natural symmetries and safeguards against the violation of the first law of thermodynamics (energy conservation).</p>

<h2 id="deriving-a-single-neuron-online-pca-using-the-reconstruction-approach">Deriving a Single-neuron Online PCA using the Reconstruction Approach</h2>
<p>To build a normative theory, we first need to derive Oja’s single-neuron online algorithm by solving an optimization problem. What objective function should we choose for online PCA? Historically, neural activity has been often viewed as representing each data sample, \({\bf x}_t\), by the feature vector, \(w\), scaled by the output, \({\bf y}_t\), Figure 2. Such reconstruction approach is naturally formalized as the minimization of the reconstruction (or coding) error:</p>

\[\min_{\| {\bf w} \|=1} {\sum \limits_{t=1}^{T} \min_{ y_t} {\| {\bf x}_t-{\bf w} y_t \| ^2_2}}. \qquad \qquad \qquad \quad (1.3)\]

<p>In the offline setting, optimization problem (1.3) is solved by PCA: the optimum w is the eigenvector of input covariance corresponding to the top eigenvalue and the optimum output, y, is the first principal component.</p>

<p>In the online setting, (1.3) can be solved by alternating minimization, which has been a subject of recent analysis. After the arrival of each data point, \({\bf x}_t\) , the algorithm computes optimum output, \({\bf y}_t\), while keeping the feature vector, \(w_{t−1}\), computed at the previous time step, fixed. By using calculus, one finds that the optimum output is given by (1.1). Then, the algorithm minimizes the total reconstruction error with respect to the feature vector while keeping all the outputs fixed. Again, resorting to calculus, one finds (1.2).</p>

<p>Thus, the single-neuron online PCA algorithm may be derived using the reconstruction approach. To compute multiple principal components, we need to extend this success to multi-neuron networks.</p>

<h2 id="the-reconstruction-approach-fails-for-multi-neuron-networks">The Reconstruction Approach Fails for Multi-neuron Networks</h2>

<p>Though the reconstruction approach yields a multi-component online PCA algorithm, the corresponding NNs are not biologically plausible.</p>

<p>Extension of the reconstruction error objective from single to multiple output components is straightforward - each scalar, \({\bf y}_t\), is replaced by a vector, \({\bf y}_t\):</p>

\[\min_{\rm diag({\bf W}^\top {\bf W})={\bf I}} {\sum \limits_{t=1}^{T} \min_{ {\bf y}_t} {\| {\bf x}_t-{\bf W} {\bf y}_t \| ^2_2}}. \qquad \qquad \qquad \quad (1.4)\]

<p>Here matrix W comprises column-vectors corresponding to different features. As in the single- neuron case this objective can be optimized online by alternating minimization. After the arrival of data sample, \({\bf x}_t\), the feature vectors are kept fixed while the objective (1.4) is minimized with respect to the principal components by iterating the following update until convergence:</p>

\[{\bf y}_t \leftarrow {\bf W} _{t-1}^\top {\bf x}_t - {\bf W} _{t-1}^\top {\bf W} _{t-1} {\bf y}_t . \qquad \qquad \qquad (1.5)\]

<p>Minimizing the total objective with respect to the feature vectors for fixed principal components yields the following update:</p>

\[{\bf W} _t \leftarrow {\bf W} _{t-1}+ \eta ( {\bf x} _t- {\bf W} _{t-1} {\bf y}_t ) {\bf y}_t^\top  \cdot \qquad \qquad (1.6)\]

<p>As before, in NN implementations of algorithm (1.5-1.6), feature vectors are represented by synaptic weights and principal components by the activities of output neurons. Then (1.5) can be implemented by a single-layer NN, Figure 3, in which activity dynamics converges faster than the time interval between the arrival of successive data samples.</p>

<p>However, implementing update (1.6) in the single-layer NN architecture, Figure 3, requires nonlocal learning rules making it biologically implausible. Indeed, the last term in (1.6) implies that updating the weight of a synapse requires the knowledge of output activities of all other neurons which are not available to the synapse. Moreover, the matrix of lateral connection weights, \(−{\bf W}^\top_{t−1}W_{t−1}\), in the last term of (1.5) is computed as a Grammian of feedforward weights, clearly a nonlocal operation. This problem is not limited to PCA and arises in networks of nonlinear neurons as well.</p>

<p>Rather than deriving learning rules from a principled objective, many authors constructed biologically plausible single-layer networks using local learning rules, Hebbian for feedforward and anti-Hebbian (meaning there is a minus sign in front of the correlation-based synaptic weight as for the last term in (1.5)). However, in my view, abandoning the normative approach creates more problems than it solves.</p>

<p>I have outlined how the conventional reconstruction approach fails to generate biologically plausible multi-neuron networks for online PCA. In the next post, I will introduce an alternative approach that overcomes this limitation. Moreover, this approach suggests a novel view of neural computation leading to many interesting extensions.</p>

<p><em>Acknowledgement: I am grateful to Sanjeev Arora for his support and encouragement as well as to Cengiz Pehlevan, Leo Shklovskii, Emily Singer, and Thomas Lin for their comments on the earlier versions.</em></p>]]></content><author><name>Dmitri 'Mitya' Chklovskii</name></author><category term="biologically-plausible" /><category term="neural-computation" /><summary type="html"><![CDATA[the conventional approach]]></summary></entry></feed>