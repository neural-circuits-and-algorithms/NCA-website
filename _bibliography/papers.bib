---
---

%% Most out of the box .bib citations should work
%% Be sure to have comma after every line as in "title={TITLE},"
%% To add url button set url={https://YOUR-URL.COM/}
%% To add pdf button set pdf={https://YOUR-URL.COM/}
%% To add code button set code={https://YOUR-URL.COM/}
%% To add an image set img={IMG_NAME.png} and place the image in /assets/img/pubs/IMG_NAME.png
%% To add a this paper to a "Related Publications" sections of a Project set related={RELATED-TAG}. RELATED-TAG should correspond to the "related-tag: *****" part of the project.md file.
%% RELATED CATEGORIES: "Theory" OR "Calcium" OR "Connectomics"
%% consider adding abstract = "Blah blah..." which generates the ABS button on layout
%% To appear on the front page of the website in the "Selected Publications" section set: selected={true},

@inproceedings{friedrich2021neural,
  title={Neural optimal feedback control with local learning rules},
  author={Johannes Friedrich and Siavash Golkar and Shiva Farashahi and Alexander Genkin and Anirvan M. Sengupta and Dmitri B. Chklovskii},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16358--16370},
  year={2021},
  abstract="A major problem in motor control is understanding how the brain plans and executes proper movements in the face of delayed and noisy stimuli. A prominent framework for addressing such control problems is Optimal Feedback Control (OFC). OFC generates control actions that optimize behaviorally relevant criteria by integrating noisy sensory stimuli and the predictions of an internal model using the Kalman filter or its extensions. However, a satisfactory neural model of Kalman filtering and control is lacking because existing proposals have the following limitations: not considering the delay of sensory feedback, training in alternating phases, requiring knowledge of the noise covariance matrices, as well as that of systems dynamics. Moreover, the majority of these studies considered Kalman filtering in isolation, and not jointly with control. To address these shortcomings, we introduce a novel online algorithm which combines adaptive Kalman filtering with a model free control approach (i.e., policy gradient algorithm). We implement this algorithm in a biologically plausible neural network with local synaptic plasticity rules. This network, with local synaptic plasticity rules, performs system identification, Kalman filtering and control with delayed noisy sensory feedback. This network performs system identification and Kalman filtering, without the need for multiple phases with distinct update rules or the knowledge of the noise covariances. It can perform state estimation with delayed sensory feedback, with the help of an internal model. It learns the control policy without requiring any knowledge of the dynamics, thus avoiding the need for weight transport. In this way, our implementation of OFC solves the credit assignment problem needed to produce the appropriate sensory-motor control in the presence of stimulus delay.",
  selected={true},
  related={Theory},
  url={https://proceedings.neurips.cc/paper/2021/hash/88591b4d3219675bdeb33584b755f680-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2021/file/88591b4d3219675bdeb33584b755f680-Paper.pdf},
  code={https://github.com/j-friedrich/neuralOFC},
  img={BioOFC.png},
  video={https://slideslive.com/38969014/neural-optimal-feedback-control-with-local-learning-rules},
}

@inproceedings{friedrich2020neuronal,
  title={Neuronal Gaussian Process Regression},
  author={Johannes Friedrich},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7090--7100},
  year={2020},
  abstract="The brain takes uncertainty intrinsic to our world into account. For example, associating spatial locations with rewards requires to predict not only expected reward at new spatial locations but also its uncertainty to avoid catastrophic events and forage safely. A powerful and flexible framework for nonlinear regression that takes uncertainty into account in a principled Bayesian manner is Gaussian process (GP) regression. Here I propose that the brain implements GP regression and present neural networks (NNs) for it. First layer neurons, e.g.\ hippocampal place cells, have tuning curves that correspond to evaluations of the GP kernel. Output neurons explicitly and distinctively encode predictive mean and variance, as observed in orbitofrontal cortex (OFC) for the case of reward prediction. Because the weights of a NN implementing exact GP regression do not arise with biological plasticity rules, I present approximations to obtain local (anti-)Hebbian synaptic learning rules. The resulting neuronal network approximates the full GP well compared to popular sparse GP approximations and achieves comparable predictive performance.",
  related={Theory},
  url={https://proceedings.neurips.cc/paper/2020/hash/4ef2f8259495563cb3a8ea4449ec4f9f-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2020/file/4ef2f8259495563cb3a8ea4449ec4f9f-Paper.pdf},
  code={https://github.com/j-friedrich/neuronalGPR},
  img={NeuronalGPR.png},
  video={https://slideslive.com/38937416/neuronal-gaussian-process-regression},
}

@article{cai2021volpy,
  title={VolPy: Automated and scalable analysis pipelines for voltage imaging datasets},
  author={Cai, Changjia and Friedrich, Johannes and Singh, Amrita and Eybposh, M Hossein and Pnevmatikakis, Eftychios A and Podgorski, Kaspar and Giovannucci, Andrea},
  journal={PLoS Computational Biology},
  volume={17},
  number={4},
  pages={e1008806},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA},
  abstract="Voltage imaging enables monitoring neural activity at sub-millisecond and sub-cellular scale, unlocking the study of subthreshold activity, synchrony, and network dynamics with unprecedented spatio-temporal resolution. However, high data rates (>800MB/s) and low signal-to-noise ratios create bottlenecks for analyzing such datasets. Here we present VolPy, an automated and scalable pipeline to pre-process voltage imaging datasets. VolPy features motion correction, memory mapping, automated segmentation, denoising and spike extraction, all built on a highly parallelizable, modular, and extensible framework optimized for memory and speed. To aid automated segmentation, we introduce a corpus of 24 manually annotated datasets from different preparations, brain areas and voltage indicators. We benchmark VolPy against ground truth segmentation, simulations and electrophysiology recordings, and we compare its performance with existing algorithms in detecting spikes. Our results indicate that VolPy’s performance in spike extraction and scalability are state-of-the-art.",
  related = {Calcium},
  img={volpy.png},
  url={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008806},
  pdf={https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1008806&type=printable}
}

@article{friedrich2021online,
  title={Online analysis of microendoscopic 1-photon calcium imaging data streams},
  author={Friedrich, Johannes and Giovannucci, Andrea and Pnevmatikakis, Eftychios A},
  journal={PLoS Computational Biology},
  volume={17},
  number={1},
  pages={e1008565},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA},
  abstract="In vivo calcium imaging through microendoscopic lenses enables imaging of neuronal populations deep within the brains of freely moving animals. Previously, a constrained matrix factorization approach (CNMF-E) has been suggested to extract single-neuronal activity from microendoscopic data. However, this approach relies on offline batch processing of the entire video data and is demanding both in terms of computing and memory requirements. These drawbacks prevent its applicability to the analysis of large datasets and closed-loop experimental settings. Here we address both issues by introducing two different online algorithms for extracting neuronal activity from streaming microendoscopic data. Our first algorithm, \on, presents an online adaptation of the CNMF-E algorithm, which dramatically reduces its memory and computation requirements. Our second algorithm proposes a convolution-based background model for microendoscopic data that enables even faster (real time) processing. Our approach is modular and can be combined with existing online motion artifact correction and activity deconvolution methods to provide a highly scalable pipeline for microendoscopic data analysis. We apply our algorithms on four previously published typical experimental datasets and show that they yield similar high-quality results as the popular offline approach, but outperform it with regard to computing time and memory requirements. They can be used instead of CNMF-E to process pre-recorded data with boosted speeds and dramatically reduced memory requirements. Further, they newly enable online analysis of live-streaming data even on a laptop.",
  related = {Calcium},
  img={friedrich2021online.png},
  url={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008565},
  pdf={https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1008565&type=printable}
}

@article{10.1162/neco_a_01476,
    author = {Tesileanu, Tiberiu and Golkar, Siavash and Nasiri, Samaneh and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
    title = "{Neural Circuits for Dynamics-Based Segmentation of Time Series}",
    journal = {Neural Computation},
    volume = {34},
    number = {4},
    pages = {891-938},
    year = {2022},
    month = {03},
    abstract = "{The brain must extract behaviorally relevant latent variables from the signals streamed by the sensory organs. Such latent variables are often encoded in the dynamics that generated the signal rather than in the specific realization of the waveform. Therefore, one problem faced by the brain is to segment time series based on underlying dynamics. We present two algorithms for performing this segmentation task that are biologically plausible, which we define as acting in a streaming setting and all learning rules being local. One algorithm is model based and can be derived from an optimization problem involving a mixture of autoregressive processes. This algorithm relies on feedback in the form of a prediction error and can also be used for forecasting future samples. In some brain regions, such as the retina, the feedback connections necessary to use the prediction error for learning are absent. For this case, we propose a second, model-free algorithm that uses a running estimate of the autocorrelation structure of the signal to perform the segmentation. We show that both algorithms do well when tasked with segmenting signals drawn from autoregressive models with piecewise-constant parameters. In particular, the segmentation accuracy is similar to that obtained from oracle-like methods in which the ground-truth parameters of the autoregressive models are known. We also test our methods on data sets generated by alternating snippets of voice recordings. We provide implementations of our algorithms at https://github.com/ttesileanu/bio-time-series.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01476},
    url = {https://doi.org/10.1162/neco\_a\_01476},
    pdf = {https://direct.mit.edu/neco/article-pdf/34/4/891/2003154/neco\_a\_01476.pdf},
    code = {https://github.com/ttesileanu/bio-time-series},
    related = {Theory},
    img = {biotime.png},
}

@article{golkar2020simple,
  title={A simple normative network approximates local non-Hebbian learning in the cortex},
  author={Golkar, Siavash and Lipshutz, David and Bahroun, Yanis and Sengupta, Anirvan and Chklovskii, Dmitri},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  img={rrr.png},
  pages={7283--7295},
  url={https://proceedings.neurips.cc/paper/2020/hash/5133aa1d673894d5a05b9d83809b9dbe-Abstract.html},
  abstract={To guide behavior, the brain extracts relevant features from high-dimensional data streamed by sensory organs. Neuroscience experiments demonstrate that the processing of sensory inputs by cortical neurons is modulated by instructive signals which provide context and task-relevant information. Here, adopting a normative approach, we model these instructive signals as supervisory inputs guiding the projection of the feedforward data. Mathematically, we start with a family of Reduced-Rank Regression (RRR) objective functions which include Reduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation Analysis (CCA), and derive novel offline and online optimization algorithms, which we call Bio-RRR. The online algorithms can be implemented by neural networks whose synaptic learning rules resemble calcium plateau potential dependent plasticity observed in the cortex. We detail how, in our model, the calcium plateau potential can be interpreted as a backpropagating error signal. We demonstrate that, despite relying exclusively on biologically plausible local learning rules, our algorithms perform competitively with existing implementations of RRMSE and CCA.},
  year={2020},
  related={Theory},
}

@article{lipshutz2020biologically,
  title={A biologically plausible neural network for slow feature analysis},
  author={Lipshutz, David and Windolf, Charles and Golkar, Siavash and Chklovskii, Dmitri B.},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  img={sfa.png},
  abstract={Learning latent features from time series data is an important problem in both machine learning and brain function. One approach, called Slow Feature Analysis (SFA), leverages the slowness of many salient features relative to the rapidly varying input signals. Furthermore, when trained on naturalistic stimuli, SFA reproduces interesting properties of cells in the primary visual cortex and hippocampus, suggesting that the brain uses temporal slowness as a computational principle for learning latent features. However, despite the potential relevance of SFA for modeling brain function, there is currently no SFA algorithm with a biologically plausible neural network implementation, by which we mean an algorithm operates in the online setting and can be mapped onto a neural network with local synaptic updates. In this work, starting from an SFA objective, we derive an SFA algorithm, called Bio-SFA, with a biologically plausible neural network implementation. We validate Bio-SFA on naturalistic stimuli.
},
  url={https://proceedings.neurips.cc/paper/2020/hash/ab73f542b6d60c4de151800b8abc0a6c-Abstract.html},
  pages={14986--14996},
  year={2020},
  related={Theory},
}

@article{giovannucci2019caiman,
  title={CaImAn: An open source tool for scalable Calcium Imaging data Analysis},
  author={Giovannucci, Andrea and Friedrich, Johannes and Gunn, Pat and Kalfon, Jeremie and Brown, Brandon L and Koay, Sue Ann and Taxidis, Jiannis and Najafi, Farzaneh and Gauthier, Jeffrey L and Zhou, Pengcheng and Khakh, Baljit S and Tank, David W and Chklovskii, Dmitri B and Pnevmatikakis, Eftychios A},
  journal={eLife},
  volume={8},
  pages={e38173},
  year={2019},
  publisher={eLife Sciences Publications Limited},
  selected={true},
  related = {Calcium},
  img={caiman.jpg},
  url = {https://elifesciences.org/articles/38173},
  code = {https://github.com/flatironinstitute/CaImAn},
  abstract = "Advances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. We present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good scalability on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected and combined a corpus of manual annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons."
}

@article{DBLP:journals/corr/abs-2112-02039,
  abbr      = {MIDL},
  author    = {Jules Berman and
               Dmitri B. Chklovskii and
               Jingpeng Wu},
  title     = {Bridging the Gap: Point Clouds for Merging Neurons in Connectomics},
  journal   = {CoRR},
  volume    = {abs/2112.02039},
  year      = {2021},
  related   = {Connectomics},
  eprinttype = {arXiv},
  eprint    = {2112.02039},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-02039.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  img       = {berman2021.png},
  url       = {https://arxiv.org/abs/2112.02039},
  pdf       = {https://arxiv.org/pdf/2112.02039.pdf},
  abstract  = "In the field of Connectomics, a primary problem is that of 3D neuron segmentation. Although deep learning-based methods have achieved remarkable accuracy, errors still exist, especially in regions with image defects. One common type of defect is that of consecutive missing image sections. Here, data is lost along some axis, and the resulting neuron segmentations are split across the gap. To address this problem, we propose a novel method based on point cloud representations of neurons. We formulate the problem as a classification problem and train CurveNet, a state-of-the-art point cloud classification model, to identify which neurons should be merged. We show that our method not only performs strongly but also scales reasonably to gaps well beyond what other methods have attempted to address. Additionally, our point cloud representations are highly efficient in terms of data, maintaining high performance with an amount of data that would be unfeasible for other methods. We believe that this is an indicator of the viability of using point cloud representations for other proofreading tasks."
}

@article{10.1162/neco_a_01414,
    abbr   = {Neural Comp},
    author = {Lipshutz, David and Bahroun, Yanis and Golkar, Siavash and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
    title = "{A Biologically Plausible Neural Network for Multichannel Canonical Correlation Analysis}",
    journal = {Neural Computation},
    volume = {33},
    number = {9},
    pages = {2309-2352},
    year = {2021},
    related={Theory},
    month = {08},
    abstract = "{Cortical pyramidal neurons receive inputs from multiple distinct neural populations and integrate these inputs in separate dendritic compartments. We explore the possibility that cortical microcircuits implement canonical correlation analysis (CCA), an unsupervised learning method that projects the inputs onto a common subspace so as to maximize the correlations between the projections. To this end, we seek a multichannel CCA algorithm that can be implemented in a biologically plausible neural network. For biological plausibility, we require that the network operates in the online setting and its synaptic update rules are local. Starting from a novel CCA objective function, we derive an online optimization algorithm whose optimization steps can be implemented in a single-layer neural network with multicompartmental neurons and local non-Hebbian learning rules. We also derive an extension of our online CCA algorithm with adaptive output rank and output whitening. Interestingly, the extension maps onto a neural network whose neural architecture and synaptic updates resemble neural circuitry and non-Hebbian plasticity observed in the cortex.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01414},
    url = {https://direct.mit.edu/neco/article/33/9/2309/102622/A-Biologically-Plausible-Neural-Network-for},
    pdf = {https://direct.mit.edu/neco/article-pdf/33/9/2309/1978152/neco\_a\_01414.pdf},
    img = {lipshutz2021.png},
}

@article{8887559,
  abbr={IEEE},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  journal={IEEE Signal Processing Magazine}, 
  title={Neuroscience-Inspired Online Unsupervised Learning Algorithms: Artificial Neural Networks}, 
  year={2019},
  volume={36},
  number={6},
  pages={88-96},
  doi={10.1109/MSP.2019.2933846},
  url={https://ieeexplore.ieee.org/document/8887559},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8887559},
  img = {neuro-insp-online.png},
  abstract= "Inventors of the original artificial neural networks (ANNs) derived their inspiration from biology [1]. However, today, most ANNs, such as backpropagation-based convolutional deeplearning networks, resemble natural NNs only superficially. Given that, on some tasks, such ANNs achieve human or even superhuman performance, why should one care about such dissimilarity with natural NNs? The algorithms of natural NNs are relevant if one's goal is not just to outperform humans on certain tasks but to develop general-purpose artificial intelligence rivaling that of a human. As contemporary ANNs are far from achieving this goal and natural NNs, by definition, achieve it, natural NNs must contain some 'secret sauce' that ANNs lack. This is why we need to understand the algorithms implemented by natural NNs.",
  }

@article{Tesileanu2021,
img = {tibi_seq_seg.png},
bibtex_show = {true},
selected = {true},
abstract = {The brain must extract behaviorally relevant latent variables from the signals streamed by the sensory organs. Such latent variables are often encoded in the dynamics that generated the signal rather than in the specific realization of the waveform. Therefore, one problem faced by the brain is to segment time series based on underlying dynamics. We present two algorithms for performing this segmentation task that are biologically plausible, which we define as acting in a streaming setting and all learning rules being local. One algorithm is model based and can be derived from an optimization problem involving a mixture of autoregressive processes. This algorithm relies on feedback in the form of a prediction error and can also be used for forecasting future samples. In some brain regions, such as the retina, the feedback connections necessary to use the prediction error for learning are absent. For this case, we propose a second, model-free algorithm that uses a running estimate of the autocorrelation structure of the signal to perform the segmentation. We show that both algorithms do well when tasked with segmenting signals drawn from autoregressive models with piecewise-constant parameters. In particular, the segmentation accuracy is similar to that obtained from oracle-like methods in which the ground-truth parameters of the autoregressive models are known. We also test our methods on data sets generated by alternating snippets of voice recordings. We provide implementations of our algorithms at <a href="https://github.com/ttesileanu/bio-time-series">https://github.com/ttesileanu/bio-time-series</a>.},
archivePrefix = {arXiv},
arxivId = {2104.11852},
author = {Tesileanu, Tiberiu and Golkar, Siavash and Nasiri, Samaneh and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
doi = {10.1162/neco_a_01476},
journal = {Neural Computation},
eprint = {2104.11852},
pages = {1--34},
title = {{Neural circuits for dynamics-based segmentation of time series}},
related = {Theory},
url = {http://arxiv.org/abs/2104.11852},
pdf = {https://arxiv.org/pdf/2104.11852.pdf},
year = {2021},
}


@article{Pehlevan_2018,
	doi = {10.1162/neco_a_01018},
	url = {https://doi.org/10.1162/neco_a_01018},
	year = 2018,
	month = {jan},
	publisher = {{MIT} Press - Journals},
	volume = {30},
	number = {1},
	pages = {84--124},
	author = {Cengiz Pehlevan and Anirvan M. Sengupta and Dmitri B. Chklovskii},
	title = {Why Do Similarity Matching Objectives Lead to Hebbian/Anti-Hebbian Networks?},
	journal = {Neural Computation},
  pdf = {https://arxiv.org/pdf/1703.07914.pdf},
  related = {Theory},
  img = {hebanti.png},
}

@article {Sengupta338947,
	author = {Sengupta, Anirvan M. and Tepper, Mariano and Pehlevan, Cengiz and Genkin, Alexander and Chklovskii, Dmitri B.},
	title = {Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks},
	elocation-id = {338947},
	year = {2018},
	doi = {10.1101/338947},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e., they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs. Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.},
	url = {https://www.biorxiv.org/content/early/2018/10/29/338947},
	pdf = {https://www.biorxiv.org/content/early/2018/10/29/338947.full.pdf},
	journal = {bioRxiv},
  img = {mani.png},
}

@article {Pehlevan226746,
	author = {Pehlevan, Cengiz and Genkin, Alexander and Chklovskii, Dmitri B.},
	title = {A clustering neural network model of insect olfaction},
	elocation-id = {226746},
	year = {2018},
	doi = {10.1101/226746},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {A key step in insect olfaction is the transformation of a dense representation of odors in a small population of neurons - projection neurons (PNs) of the antennal lobe - into a sparse representation in a much larger population of neurons -Kenyon cells (KCs) of the mushroom body. What computational purpose does this transformation serve? We propose that the PN-KC network implements an online clustering algorithm which we derive from the k-means cost function. The vector of PN-KC synaptic weights converging onto a given KC represents the corresponding cluster centroid. KC activities represent attribution indices, i.e. the degree to which a given odor presentation is attributed to each cluster. Remarkably, such clustering view of the PN-KC circuit naturally accounts for several of its salient features. First, attribution indices are nonnegative thus rationalizing rectification in KCs. Second, the constraint on the total sum of attribution indices for each presentation is enforced by a Lagrange multiplier identified with the activity of a single inhibitory interneuron reciprocally connected with KCs. Third, the soft-clustering version of our algorithm reproduces observed sparsity and overcompleteness of the KC representation which may optimize supervised classification downstream.},
	url = {https://www.biorxiv.org/content/early/2018/01/27/226746},
	pdf = {https://www.biorxiv.org/content/early/2018/01/27/226746.full.pdf},
	journal = {bioRxiv},
  img = {insect_clustering.png},
}

@inproceedings{giovannucci2017OnACID,
  author = {Giovannucci, Andrea and Friedrich, Johannes and Kaufman, Matt and Churchland, Anne and Chklovskii, Dmitri and Paninski, Liam and Pnevmatikakis, Eftychios A},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {OnACID: Online Analysis of Calcium Imaging Data in Real Time},
  volume = {30},
  year = {2017},
  abstract="Optical imaging methods using calcium indicators are critical for monitoring the activity of large neuronal populations in vivo. Imaging experiments typically generate a large amount of data that needs to be processed to extract the activity of the imaged neuronal sources. While deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of data at a time, rendering them vulnerable to the volume of the recorded data, and preventing real-time experimental interrogation. Here we introduce OnACID, an Online framework for the Analysis of streaming Calcium Imaging Data, including i) motion artifact correction, ii) neuronal source extraction, and iii) activity denoising and deconvolution. Our approach combines and extends previous work on online dictionary learning and calcium imaging data analysis, to deliver an automated pipeline that can discover and track the activity of hundreds of cells in real time, thereby enabling new types of closed-loop experiments. We apply our algorithm on two large scale experimental datasets, benchmark its performance on manually annotated data, and show that it outperforms a popular offline approach.",
  selected={true},
  related={Calcium},
  url = {https://papers.nips.cc/paper/2017/hash/4edaa105d5f53590338791951e38c3ad-Abstract.html},
  pdf = {https://proceedings.neurips.cc/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf},
  img={OnACID.png},
}

@article{pnevmatikakis2017normcorre,
  title={NoRMCorre: An online algorithm for piecewise rigid motion correction of calcium imaging data},
  author={Pnevmatikakis, Eftychios A and Giovannucci, Andrea},
  journal={Journal of Neuroscience Methods},
  volume={291},
  pages={83--94},
  year={2017},
  publisher={Elsevier},
  abstract="Background: 
Motion correction is a challenging pre-processing problem that arises early in the analysis pipeline of calcium imaging data sequences. The motion artifacts in two-photon microscopy recordings can be non-rigid, arising from the finite time of raster scanning and non-uniform deformations of the brain medium.
New method: 
We introduce an algorithm for fast Non-Rigid Motion Correction (NoRMCorre) based on template matching. NoRMCorre operates by splitting the field of view (FOV) into overlapping spatial patches along all directions. The patches are registered at a sub-pixel resolution for rigid translation against a regularly updated template. The estimated alignments are subsequently up-sampled to create a smooth motion field for each frame that can efficiently approximate non-rigid artifacts in a piecewise-rigid manner.
Existing methods: 
Existing approaches either do not scale well in terms of computational performance or are targeted to non-rigid artifacts arising just from the finite speed of raster scanning, and thus cannot correct for non-rigid motion observable in datasets from a large FOV.
Results: 
NoRMCorre can be run in an online mode resulting in comparable to or even faster than real time motion registration of streaming data. We evaluate its performance with simple yet intuitive metrics and compare against other non-rigid registration methods on simulated data and in vivo two-photon calcium imaging datasets. Open source Matlab and Python code is also made available.
Conclusions: 
The proposed method and accompanying code can be useful for solving large scale image registration problems in calcium imaging, especially in the presence of non-rigid deformations.",
  related = {Calcium},
  img={NoRMCorre.jpg},
  code={https://github.com/flatironinstitute/NoRMCorre},
  url={https://www.sciencedirect.com/science/article/pii/S0165027017302753},
  pdf={https://reader.elsevier.com/reader/sd/pii/S0165027017302753?token=608390C16B5A2B7F7164DCCB91D865D32809CC12C6837E582BD51BBB3203B21B967B24C5E913A62B21F81FB6712D5AFE&originRegion=us-east-1&originCreation=20220707192104}
}

@article{Zador_2023,
	title = {Catalyzing next-generation Artificial Intelligence through {NeuroAI}},
	doi = {10.1038/s41467-023-37180-x},
	url = {https://doi.org/10.1038/s41467-023-37180-x},
	year = {2023},
	month = {mar},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {14},
	number = {1},
	author = {Anthony Zador and Sean Escola and Blake Richards and Bence Ölveczky and Yoshua Bengio and Kwabena Boahen and Matthew Botvinick and Dmitri Chklovskii and Anne Churchland and Claudia Clopath and James DiCarlo and Surya Ganguli and Jeff Hawkins and Konrad Körding and Alexei Koulakov and Yann LeCun and Timothy Lillicrap and Adam Marblestone and Bruno Olshausen and Alexandre Pouget and Cristina Savin and Terrence Sejnowski and Eero Simoncelli and Sara Solla and David Sussillo and Andreas S. Tolias and Doris Tsao},
        img={embodied_turing_test.png},
	journal = {Nature Communications},
        related = {Theory}
}

@article{lipshutz2023normative,
      title={A normative framework for deriving neural networks with multi-compartmental neurons and non-Hebbian plasticity}, 
      author={David Lipshutz and Yanis Bahroun and Siavash Golkar and Anirvan M. Sengupta and Dmitri B. Chklovskii},
      url = {https://arxiv.org/abs/2302.10051},
      year={2023},
      eprint={2302.10051},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      img={deriving.png},
      related = {Theory}
}

@article{Lipshutz_2023,
	doi = {10.1371/journal.pcbi.1010864},
	url = {https://doi.org/10.1371/journal.pcbi.1010864},
	year = 2023,
	month = {feb},
	publisher = {Public Library of Science ({PLoS})},
	volume = {19},
	number = {2},
	pages = {e1010864},
	author = {David Lipshutz and Aneesh Kashalikar and Shiva Farashahi and Dmitri B. Chklovskii},
	editor = {Michele Migliore},
	title = {A linear discriminant analysis model of imbalanced associative learning in the mushroom body compartment},
	journal = {{PLOS} Computational Biology},
        img={linear_discriminant_analysis_model.png},
        related = {Theory}
}

@article{duong2023statistical,
      title={Statistical whitening of neural populations with gain-modulating interneurons}, 
      url = {https://arxiv.org/abs/2301.11955},
      author={Lyndon R. Duong and David Lipshutz and David J. Heeger and Dmitri B. Chklovskii and Eero P. Simoncelli},
      year={2023},
      eprint={2301.11955},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      img={statistical_whitening.png},
      related = {Theory}
}

@article{Qin_2023,
	doi = {10.1038/s41593-022-01225-z},
	url = {https://doi.org/10.1038/s41593-022-01225-z},
	year = 2023,
	month = {jan},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {26},
	number = {2},
	pages = {339--349},
	author = {Shanshan Qin and Shiva Farashahi and David Lipshutz and Anirvan M. Sengupta and Dmitri B. Chklovskii and Cengiz Pehlevan},
	title = {Coordinated drift of receptive fields in Hebbian/anti-Hebbian network models during noisy representation learning},
	journal = {Nature Neuroscience},
        img={drift_of_receptive_fields.png},
        related = {Theory}
}

@inproceedings{NEURIPS2022_834f4c0b,
 author = {Genkin, Alexander and Lipshutz, David and Golkar, Siavash and Tesileanu, Tiberiu and Chklovskii, Dmitri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20838--20849},
 publisher = {Curran Associates, Inc.},
 title = {Biological Learning of Irreducible Representations of Commuting Transformations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/834f4c0b8d241b4943a9dcb77fd85675-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 img={irreducible_representations.png},
 related = {Theory}
}

@inproceedings{NEURIPS2022_5b5de852,
 author = {Golkar, Siavash and Tesileanu, Tiberiu and Bahroun, Yanis and Sengupta, Anirvan and Chklovskii, Dmitri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {14155--14169},
 publisher = {Curran Associates, Inc.},
 title = {Constrained Predictive Coding as a Biologically Plausible Model of the Cortical Hierarchy},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5b5de8526aac159e37ff9547713677ed-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 img={constrained_predictive_coding.png},
 related = {Theory}
}

@article{golkar2022online,
      title={An online algorithm for contrastive Principal Component Analysis},
      url={https://arxiv.org/abs/2211.07723},
      author={Siavash Golkar and David Lipshutz and Tiberiu Tesileanu and Dmitri B. Chklovskii},
      year={2022},
      eprint={2211.07723},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      img={contrastive_pca.png},
      related = {Theory}
}

@article{lipshutz2022interneurons,
      title={Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation},
      url={https://arxiv.org/abs/2209.10634},
      author={David Lipshutz and Cengiz Pehlevan and Dmitri B. Chklovskii},
      year={2022},
      eprint={2209.10634},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      img={interneurons_learning_dynamics.png},
      related = {Theory}
}

@article{Lipshutz_2022,
	doi = {10.1007/s00422-022-00943-8},
	url = {https://doi.org/10.1007/s00422-022-00943-8},
	year = 2022,
	month = {sep},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {116},
	number = {5-6},
	pages = {557--568},
	author = {David Lipshutz and Cengiz Pehlevan and Dmitri B. Chklovskii},
	title = {Biologically plausible single-layer networks for nonnegative independent component analysis},
	journal = {Biological Cybernetics},
        img={single_layer_nnica.png},
        related = {Theory}
}

@article{10.1162/neco_a_01476,
    author = {Teşileanu, Tiberiu and Golkar, Siavash and Nasiri, Samaneh and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
    title = "{Neural Circuits for Dynamics-Based Segmentation of Time Series}",
    journal = {Neural Computation},
    volume = {34},
    number = {4},
    pages = {891-938},
    year = {2022},
    month = {03},
    abstract = "{The brain must extract behaviorally relevant latent variables from the signals streamed by the sensory organs. Such latent variables are often encoded in the dynamics that generated the signal rather than in the specific realization of the waveform. Therefore, one problem faced by the brain is to segment time series based on underlying dynamics. We present two algorithms for performing this segmentation task that are biologically plausible, which we define as acting in a streaming setting and all learning rules being local. One algorithm is model based and can be derived from an optimization problem involving a mixture of autoregressive processes. This algorithm relies on feedback in the form of a prediction error and can also be used for forecasting future samples. In some brain regions, such as the retina, the feedback connections necessary to use the prediction error for learning are absent. For this case, we propose a second, model-free algorithm that uses a running estimate of the autocorrelation structure of the signal to perform the segmentation. We show that both algorithms do well when tasked with segmenting signals drawn from autoregressive models with piecewise-constant parameters. In particular, the segmentation accuracy is similar to that obtained from oracle-like methods in which the ground-truth parameters of the autoregressive models are known. We also test our methods on data sets generated by alternating snippets of voice recordings. We provide implementations of our algorithms at https://github.com/ttesileanu/bio-time-series.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01476},
    url = {https://doi.org/10.1162/neco\_a\_01476},
    eprint = {https://direct.mit.edu/neco/article-pdf/34/4/891/2003154/neco\_a\_01476.pdf},
    img={circuits_for_dynamics_segmentation.png},
    related = {Theory}
}

@inproceedings{NEURIPS2021_3ce3bd7d,
 author = {Bahroun, Yanis and Chklovskii, Dmitri and Sengupta, Anirvan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7368--7384},
 publisher = {Curran Associates, Inc.},
 title = {A Normative and Biologically Plausible Algorithm for Independent Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf},
 volume = {34},
 year = {2021},
 img={ica_algo.png},
 related = {Theory}
}

@article {Chapochnikov2021.09.24.461723,
        author = {Nikolai M. Chapochnikov and Cengiz Pehlevan and Dmitri B. Chklovskii},
        title = {Normative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction},
        elocation-id = {2021.09.24.461723},
        year = {2021},
        doi = {10.1101/2021.09.24.461723},
        publisher = {Cold Spring Harbor Laboratory},
        abstract = {One major question in neuroscience is how to relate connectomes to neural activity, circuit function, and learning. We offer an answer in the peripheral olfactory circuit of the Drosophila larva, composed of olfactory receptor neurons (ORNs) connected through feedback loops with interconnected inhibitory local neurons (LNs). We combine structural and activity data and, using a holistic normative framework based on similarity-matching, we propose a biologically plausible mechanistic model of the circuit. Our model predicts the ORN {\textrightarrow} LN synaptic weights found in the connectome and demonstrate that they reflect correlations in ORN activity patterns. Additionally, our model explains the relation between ORN {\textrightarrow} LN and LN {\textendash} LN synaptic weight and the arising of different LN types. This global synaptic organization can autonomously arise through Hebbian plasticity, and thus allows the circuit to adapt to different environments in an unsupervised manner. Functionally, we propose LNs extract redundant input correlations and dampen them in ORNs, thus partially whitening and normalizing the stimulus representations in ORNs. Our work proposes a comprehensive framework to combine structure, activity, function, and learning, and uncovers a general and potent circuit motif that can learn and extract significant input features and render stimulus representations more efficient.Significance The brain represents information with patterns of neural activity. At the periphery, due to the properties of the external world and of encoding neurons, these patterns contain correlations, which are detrimental for stimulus discrimination. We study the peripheral olfactory neural circuit of the Drosophila larva, that preprocesses neural representations before relaying them to higher brain areas. A comprehensive understanding of this preprocessing is, however, lacking. Here, we propose a mechanistic and normative framework describing the function of the circuit and predict the circuit{\textquoteright}s synaptic organization based on the circuit{\textquoteright}s input neural activity. We show how the circuit can autonomously adapt to different environments, extracts stimulus features, and decorrelate and normalize input representations, which facilitates odor discrimination downstream.Competing Interest StatementThe authors have declared no competing interest.},
        URL = {https://www.biorxiv.org/content/early/2021/09/25/2021.09.24.461723},
        eprint = {https://www.biorxiv.org/content/early/2021/09/25/2021.09.24.461723.full.pdf},
        journal = {bioRxiv},
        img={efficient_circuit_model.png},
        related = {Theory}
}

@article{MAKAROVA202177,
title = {Small brains for big science},
journal = {Current Opinion in Neurobiology},
volume = {71},
pages = {77-83},
year = {2021},
note = {Evolution of Brains and Computation},
issn = {0959--4388},
doi = {https://doi.org/10.1016/j.conb.2021.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001021},
author = {Anastasia A. Makarova and Alexey A. Polilov and Dmitri B. Chklovskii},
abstract = {As the study of the human brain is complicated by its sheer scale, complexity, and impracticality of invasive experiments, neuroscience research has long relied on model organisms. The brains of macaque, mouse, zebrafish, fruit fly, nematode, and others have yielded many secrets that advanced our understanding of the human brain. Here, we propose that adding miniature insects to this collection would reduce the costs and accelerate brain research. The smallest insects occupy a special place among miniature animals: despite their body sizes, comparable to unicellular organisms, they retain complex brains that include thousands of neurons. Their brains possess the advantages of those in insects, such as neuronal identifiability and the connectome stereotypy, yet are smaller and hence easier to map and understand. Finally, the brains of miniature insects offer insights into the evolution of brain design.},
img={small_brains_for_big_science.png},
related = {Connectomics}
}

@misc{wu2023outofdomain,
      title={An Out-of-Domain Synapse Detection Challenge for Microwasp Brain Connectomes},
      url = {https://arxiv.org/abs/2302.00545},
      author={Jingpeng Wu and Yicong Li and Nishika Gupta and Kazunori Shinomiya and Pat Gunn and Alexey Polilov and Hanspeter Pfister and Dmitri Chklovskii and Donglai Wei},
      year={2023},
      eprint={2302.00545},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      img={synapse_detection_challenge.png},
      related = {Connectomics}
}

@article{CHUA2023,
title = {A complete reconstruction of the early visual system of an adult insect},
journal = {Current Biology},
year = {2023},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2023.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S096098222301237X},
author = {Nicholas J. Chua and Anastasia A. Makarova and Pat Gunn and Sonia Villani and Ben Cohen and Myisha Thasin and Jingpeng Wu and Deena Shefter and Song Pang and C. Shan Xu and Harald F. Hess and Alexey A. Polilov and Dmitri B. Chklovskii},
abstract = {For most model organisms in neuroscience, research into visual processing in the brain is difficult because of a lack of high-resolution maps that capture complex neuronal circuitry. The microinsect Megaphragma viggianii, because of its small size and non-trivial behavior, provides a unique opportunity for tractable whole-organism connectomics. We image its whole head using serial electron microscopy. We reconstruct its compound eye and analyze the optical properties of the ommatidia as well as the connectome of the first visual neuropil—the lamina. Compared with the fruit fly and the honeybee, Megaphragma visual system is highly simplified: it has 29 ommatidia per eye and 6 lamina neuron types. We report features that are both stereotypical among most ommatidia and specialized to some. By identifying the “barebones” circuits critical for flying insects, our results will facilitate constructing computational models of visual processing in insects.},
img={complete_reconstruction_of_early_visual_system.jpg},
related={Connectomics}
}

@misc{duong2023adaptive,
      title={Adaptive whitening with fast gain modulation and slow synaptic plasticity}, 
      author={Lyndon R. Duong and Eero P. Simoncelli and Dmitri B. Chklovskii and David Lipshutz},
      year={2023},
      eprint={2308.13633},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      url={https://arxiv.org/abs/2308.13633},
      img={adaptive_whitening_with_fast_gain_modulation.jpg},
      related={Theory}
}

@article{bahroun2023duality,
  title={Duality Principle and Biologically Plausible Learning: Connecting the Representer Theorem and Hebbian Learning},
  author={Bahroun, Yanis and Chklovskii, Dmitri B and Sengupta, Anirvan M},
  journal={arXiv preprint arXiv:2309.16687},
  year={2023},
  url={https://arxiv.org/abs/2309.16687},
  related={Theory}
}

@misc{bahroun2023unlocking,
      title={Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training}, 
      author={Yanis Bahroun and Shagesh Sridharan and Atithi Acharya and Dmitri B. Chklovskii and Anirvan M. Sengupta},
      year={2023},
      eprint={2308.02427},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2308.02427},
      img={potential_of_similarity_matching.jpg},
      related={Theory}
}

@article{2023PNAS..12017484C,
       author = {{Chapochnikov}, Nikolai M. and {Pehlevan}, Cengiz and {Chklovskii}, Dmitri B.},
        title = "{Normative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction}",
      journal = {Proceedings of the National Academy of Science},
         year = 2023,
        month = jul,
       volume = {120},
       number = {29},
          eid = {e2117484120},
        pages = {e2117484120},
          doi = {10.1073/pnas.2117484120},
          url = {https://www.pnas.org/doi/10.1073/pnas.2117484120},
      img = {normative_and_mechanistic_model.jpg},
      related = {Theory}
}

@INPROCEEDINGS{10096380,
  author={Golkar, Siavash and Lipshutz, David and Tesileanu, Tiberiu and Chklovskii, Dmitri B.},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={An Online Algorithm for Contrastive Principal Component Analysis}, 
  year={2023},
  pages={1-5},
  url={https://ieeexplore.ieee.org/document/10096380},
  doi={10.1109/ICASSP49357.2023.10096380},
  related={Theory}
}

@article {Moore2024.01.02.573843,
	author = {Jason Moore and Alexander Genkin and Magnus Tournoy and Joshua Pughe-Sanford and Rob R de Ruyter van Steveninck and Dmitri B Chklovskii},
	title = {The Neuron as a Direct Data-Driven Controller},
	elocation-id = {2024.01.02.573843},
	year = {2024},
	doi = {10.1101/2024.01.02.573843},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {In the quest to model neuronal function amidst gaps in physiological data, a promising strategy is to develop a normative theory that interprets neuronal physiology as optimizing a computational objective. This study extends the current normative models, which primarily optimize prediction, by conceptualizing neurons as optimal feedback controllers. We posit that neurons, especially those beyond early sensory areas, act as controllers, steering their environment towards a specific desired state through their output. This environment comprises both synaptically interlinked neurons and external motor sensory feedback loops, enabling neurons to evaluate the effectiveness of their control via synaptic feedback. Utilizing the novel Direct Data-Driven Control (DD-DC) framework, we model neurons as biologically feasible controllers which implicitly identify loop dynamics, infer latent states and optimize control. Our DD-DC neuron model explains various neurophysiological phenomena: the shift from potentiation to depression in Spike-Timing-Dependent Plasticity (STDP) with its asymmetry, the duration and adaptive nature of feedforward and feedback neuronal filters, the imprecision in spike generation under constant stimulation, and the characteristic operational variability and noise in the brain. Our model presents a significant departure from the traditional, feedforward, instant-response McCulloch-Pitts-Rosenblatt neuron, offering a novel and biologically-informed fundamental unit for constructing neural networks.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2024/01/03/2024.01.02.573843},
	eprint = {https://www.biorxiv.org/content/early/2024/01/03/2024.01.02.573843.full.pdf},
	journal = {bioRxiv},
        related={Theory},
        img={DDDC.png}
}

@article{PhysRevResearch.6.013111,
  title = {Neuronal temporal filters as normal mode extractors},
  author = {Golkar, Siavash and Berman, Jules and Lipshutz, David and Haret, Robert Mihai and Gollisch, Tim and Chklovskii, Dmitri B.},
  journal = {Phys. Rev. Res.},
  volume = {6},
  issue = {1},
  pages = {013111},
  numpages = {8},
  year = {2024},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.6.013111},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.013111},
  related={Theory},
  img={neuronal_temporal_filters.jpg}
}

