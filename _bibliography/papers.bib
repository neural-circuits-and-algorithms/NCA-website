---
---

%% Most out of the box .bib citations should work
%% Be sure to have comma after every line as in "title={TITLE},"
%% To add url button set url={https://YOUR-URL.COM/}
%% To add pdf button set pdf={https://YOUR-URL.COM/}
%% To add code button set code={https://YOUR-URL.COM/}
%% To add an image set img={IMG_NAME.png} and place the image in /assets/img/pubs/IMG_NAME.png
%% To add a this paper to a "Related Publications" sections of a Project set related={RELATED-TAG}. RELATED-TAG should correspond to the "related-tag: *****" part of the project.md file.
%% RELATED CATEGORIES: "Theory" OR "Calcium" OR "Connectomics"
%% consider adding abstract = "Blah blah..." which generates the ABS button on layout
%% To appear on the front page of the website in the "Selected Publications" section set: selected={true},

@article{golkar2020simple,
  title={A simple normative network approximates local non-Hebbian learning in the cortex},
  author={Golkar, Siavash and Lipshutz, David and Bahroun, Yanis and Sengupta, Anirvan and Chklovskii, Dmitri},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  img={rrr.png},
  pages={7283--7295},
  url={https://proceedings.neurips.cc/paper/2020/hash/5133aa1d673894d5a05b9d83809b9dbe-Abstract.html},
  abstract={To guide behavior, the brain extracts relevant features from high-dimensional data streamed by sensory organs. Neuroscience experiments demonstrate that the processing of sensory inputs by cortical neurons is modulated by instructive signals which provide context and task-relevant information. Here, adopting a normative approach, we model these instructive signals as supervisory inputs guiding the projection of the feedforward data. Mathematically, we start with a family of Reduced-Rank Regression (RRR) objective functions which include Reduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation Analysis (CCA), and derive novel offline and online optimization algorithms, which we call Bio-RRR. The online algorithms can be implemented by neural networks whose synaptic learning rules resemble calcium plateau potential dependent plasticity observed in the cortex. We detail how, in our model, the calcium plateau potential can be interpreted as a backpropagating error signal. We demonstrate that, despite relying exclusively on biologically plausible local learning rules, our algorithms perform competitively with existing implementations of RRMSE and CCA.
},
  year={2020},
  related={Theory},
}

@article{lipshutz2020biologically,
  title={A biologically plausible neural network for slow feature analysis},
  author={Lipshutz, David and Windolf, Charles and Golkar, Siavash and Chklovskii, Dmitri B.},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  img={sfa.png},
  abstract={Learning latent features from time series data is an important problem in both machine learning and brain function. One approach, called Slow Feature Analysis (SFA), leverages the slowness of many salient features relative to the rapidly varying input signals. Furthermore, when trained on naturalistic stimuli, SFA reproduces interesting properties of cells in the primary visual cortex and hippocampus, suggesting that the brain uses temporal slowness as a computational principle for learning latent features. However, despite the potential relevance of SFA for modeling brain function, there is currently no SFA algorithm with a biologically plausible neural network implementation, by which we mean an algorithm operates in the online setting and can be mapped onto a neural network with local synaptic updates. In this work, starting from an SFA objective, we derive an SFA algorithm, called Bio-SFA, with a biologically plausible neural network implementation. We validate Bio-SFA on naturalistic stimuli.
},
  url={https://proceedings.neurips.cc/paper/2020/hash/ab73f542b6d60c4de151800b8abc0a6c-Abstract.html},
  pages={14986--14996},
  year={2020},
  related={Theory},
}

@misc{friedrich2021neural,
      title={Neural optimal feedback control with local learning rules}, 
      author={Johannes Friedrich and Siavash Golkar and Shiva Farashahi and Alexander Genkin and Anirvan M. Sengupta and Dmitri B. Chklovskii},
      year={2021},
      eprint={2111.06920},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      selected={true},
      related={Theory},
}

@article{giovannucci2019caiman,
  title={CaImAn: An open source tool for scalable Calcium Imaging data Analysis},
  author={Giovannucci, Andrea and Friedrich, Johannes and Gunn, Pat and Kalfon, Jeremie and Brown, Brandon L and Koay, Sue Ann and Taxidis, Jiannis and Najafi, Farzaneh and Gauthier, Jeffrey L and Zhou, Pengcheng and Khakh, Baljit S and Tank, David W and Chklovskii, Dmitri B and Pnevmatikakis, Eftychios A},
  journal={eLife},
  volume={8},
  pages={e38173},
  year={2019},
  publisher={eLife Sciences Publications Limited},
  related = {Calcium},
  img={caiman.jpg},
  url = {https://elifesciences.org/articles/38173},
  code = {https://github.com/flatironinstitute/CaImAn},
  abstract = "Advances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. We present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good scalability on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected and combined a corpus of manual annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons."
}

@article{DBLP:journals/corr/abs-2112-02039,
  abbr      = {MIDL},
  author    = {Jules Berman and
               Dmitri B. Chklovskii and
               Jingpeng Wu},
  title     = {Bridging the Gap: Point Clouds for Merging Neurons in Connectomics},
  journal   = {CoRR},
  volume    = {abs/2112.02039},
  year      = {2021},
  related   = {Connectomics},
  eprinttype = {arXiv},
  eprint    = {2112.02039},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-02039.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  img       = {berman2021.png},
  url       = {https://arxiv.org/abs/2112.02039},
  pdf       = {https://arxiv.org/pdf/2112.02039.pdf},
  abstract  = "In the field of Connectomics, a primary problem is that of 3D neuron segmentation. Although deep learning-based methods have achieved remarkable accuracy, errors still exist, especially in regions with image defects. One common type of defect is that of consecutive missing image sections. Here, data is lost along some axis, and the resulting neuron segmentations are split across the gap. To address this problem, we propose a novel method based on point cloud representations of neurons. We formulate the problem as a classification problem and train CurveNet, a state-of-the-art point cloud classification model, to identify which neurons should be merged. We show that our method not only performs strongly but also scales reasonably to gaps well beyond what other methods have attempted to address. Additionally, our point cloud representations are highly efficient in terms of data, maintaining high performance with an amount of data that would be unfeasible for other methods. We believe that this is an indicator of the viability of using point cloud representations for other proofreading tasks."
}

@article{10.1162/neco_a_01414,
    abbr   = {Neural Comp},
    author = {Lipshutz, David and Bahroun, Yanis and Golkar, Siavash and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
    title = "{A Biologically Plausible Neural Network for Multichannel Canonical Correlation Analysis}",
    journal = {Neural Computation},
    volume = {33},
    number = {9},
    pages = {2309-2352},
    year = {2021},
    related={Theory},
    month = {08},
    abstract = "{Cortical pyramidal neurons receive inputs from multiple distinct neural populations and integrate these inputs in separate dendritic compartments. We explore the possibility that cortical microcircuits implement canonical correlation analysis (CCA), an unsupervised learning method that projects the inputs onto a common subspace so as to maximize the correlations between the projections. To this end, we seek a multichannel CCA algorithm that can be implemented in a biologically plausible neural network. For biological plausibility, we require that the network operates in the online setting and its synaptic update rules are local. Starting from a novel CCA objective function, we derive an online optimization algorithm whose optimization steps can be implemented in a single-layer neural network with multicompartmental neurons and local non-Hebbian learning rules. We also derive an extension of our online CCA algorithm with adaptive output rank and output whitening. Interestingly, the extension maps onto a neural network whose neural architecture and synaptic updates resemble neural circuitry and non-Hebbian plasticity observed in the cortex.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01414},
    url = {https://direct.mit.edu/neco/article/33/9/2309/102622/A-Biologically-Plausible-Neural-Network-for},
    pdf = {https://direct.mit.edu/neco/article-pdf/33/9/2309/1978152/neco\_a\_01414.pdf},
    img = {lipshutz2021.png},
}

@article{8887559,
  abbr={IEEE},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  journal={IEEE Signal Processing Magazine}, 
  title={Neuroscience-Inspired Online Unsupervised Learning Algorithms: Artificial Neural Networks}, 
  year={2019},
  volume={36},
  number={6},
  pages={88-96},
  doi={10.1109/MSP.2019.2933846},
  url={https://ieeexplore.ieee.org/document/8887559},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8887559},
  abstract= "Inventors of the original artificial neural networks (ANNs) derived their inspiration from biology [1]. However, today, most ANNs, such as backpropagation-based convolutional deeplearning networks, resemble natural NNs only superficially. Given that, on some tasks, such ANNs achieve human or even superhuman performance, why should one care about such dissimilarity with natural NNs? The algorithms of natural NNs are relevant if one's goal is not just to outperform humans on certain tasks but to develop general-purpose artificial intelligence rivaling that of a human. As contemporary ANNs are far from achieving this goal and natural NNs, by definition, achieve it, natural NNs must contain some 'secret sauce' that ANNs lack. This is why we need to understand the algorithms implemented by natural NNs.",
  }

@article{Tesileanu2021,
img = {tibi_seq_seg.png},
bibtex_show = {true},
selected = {true},
abstract = {The brain must extract behaviorally relevant latent variables from the signals streamed by the sensory organs. Such latent variables are often encoded in the dynamics that generated the signal rather than in the specific realization of the waveform. Therefore, one problem faced by the brain is to segment time series based on underlying dynamics. We present two algorithms for performing this segmentation task that are biologically plausible, which we define as acting in a streaming setting and all learning rules being local. One algorithm is model based and can be derived from an optimization problem involving a mixture of autoregressive processes. This algorithm relies on feedback in the form of a prediction error and can also be used for forecasting future samples. In some brain regions, such as the retina, the feedback connections necessary to use the prediction error for learning are absent. For this case, we propose a second, model-free algorithm that uses a running estimate of the autocorrelation structure of the signal to perform the segmentation. We show that both algorithms do well when tasked with segmenting signals drawn from autoregressive models with piecewise-constant parameters. In particular, the segmentation accuracy is similar to that obtained from oracle-like methods in which the ground-truth parameters of the autoregressive models are known. We also test our methods on data sets generated by alternating snippets of voice recordings. We provide implementations of our algorithms at <a href="https://github.com/ttesileanu/bio-time-series">https://github.com/ttesileanu/bio-time-series</a>.},
archivePrefix = {arXiv},
arxivId = {2104.11852},
author = {Tesileanu, Tiberiu and Golkar, Siavash and Nasiri, Samaneh and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
doi = {10.1162/neco_a_01476},
journal = {arXiv},
eprint = {2104.11852},
pages = {1--34},
title = {{Neural circuits for dynamics-based segmentation of time series}},
keywords = {biowta},
related = {Theory},
url = {http://arxiv.org/abs/2104.11852},
pdf = {https://arxiv.org/pdf/2104.11852.pdf},
year = {2021},
}
